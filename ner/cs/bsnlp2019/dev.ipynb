{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annt_file(annt_file_path):\n",
    "    # return tuples of (entity_text, entity_type)\n",
    "    f = open(annt_file_path, 'r', encoding='utf8')\n",
    "    result = []\n",
    "    for l in f:\n",
    "        l = l.replace('\\n', '').strip()\n",
    "        parts = l.split('\\t')\n",
    "        if len(parts) == 4:\n",
    "            text = parts[0]\n",
    "            ent_type = parts[2]\n",
    "            result.append((text, ent_type))\n",
    "    f.close()\n",
    "    return result\n",
    "    \n",
    "# apath = 'original/test/annotated/nord-stream_CZ-new.xml_file_30000.out'\n",
    "# parse_annt_file(apath)\n",
    "\n",
    "def parse_raw_file(raw_file_path, stanza_nlp):\n",
    "    # return a list of sentences\n",
    "    started = False\n",
    "    result = []\n",
    "    f = open(raw_file_path, 'r', encoding='utf8')\n",
    "    for l in f:\n",
    "        if started:\n",
    "            l = l.replace('\\n', '').strip()\n",
    "            if len(l) > 1:\n",
    "                doc = stanza_nlp(l)\n",
    "                for sent in doc.sentences:\n",
    "                    result.append(sent.text)\n",
    "        if 'http://' in l or 'https://' in l:\n",
    "            started = True\n",
    "    f.close()\n",
    "    return result\n",
    "\n",
    "# rpath = 'original/test/raw/nord_stream_cs.txt_file_3.txt'\n",
    "# parse_raw_file(rpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sents_and_ents(stanza_nlp, spacy_nlp, raw_file_path, annt_file_path):\n",
    "    '''\n",
    "    params:\n",
    "    -----------------\n",
    "    sents: ['this is a sentence.', 'another one?']\n",
    "    ents: [(phrase1, type1), (phrase2, type2)]\n",
    "    \n",
    "    returns:\n",
    "    -----------------\n",
    "    tokenized and annotated sentences:\n",
    "    [[(t1, annt1), (t2, annt2)], [(t3, annt3), (t4, annt4), (t5, annt5)]]\n",
    "    '''\n",
    "    def find_ent(sent, ents):\n",
    "        result = {}\n",
    "        for ent_text, ent_type in ents:\n",
    "            if ent_text in sent:\n",
    "                start = sent.index(ent_text)\n",
    "                end = start + len(ent_text)\n",
    "                result[(start, end)] = ent_type\n",
    "        return result\n",
    "    \n",
    "    sents = parse_raw_file(raw_file_path, stanza_nlp) \n",
    "    ents = parse_annt_file(annt_file_path)\n",
    "    result = []\n",
    "    for sent in sents:\n",
    "        sent_arr = []\n",
    "        _dict = find_ent(sent, ents)\n",
    "        doc = spacy_nlp(sent)\n",
    "        for t in doc:\n",
    "            label = 'O'\n",
    "            if len(_dict) > 0:\n",
    "                for start, end in _dict.keys():\n",
    "                    if t.idx >=start and t.idx <end:\n",
    "                        head = 'B-' if t.idx == start else 'I-'\n",
    "                        label = head + _dict[(start, end)]\n",
    "                        break\n",
    "            sent_arr.append((t.text, label))\n",
    "        result.append(sent_arr)\n",
    "    return result       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ner_bio_sents(folder='test'):\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    import stanza\n",
    "    from spacy_stanza import StanzaLanguage\n",
    "    snlp = stanza.Pipeline(lang=\"cs\", logging_level='ERROR')\n",
    "    nlp = StanzaLanguage(snlp)\n",
    "\n",
    "    annt_path = os.path.join('original', folder, 'annotated')\n",
    "    raw_path = os.path.join('original', folder, 'raw')\n",
    "    annts = sorted(os.listdir(annt_path))\n",
    "    annts = list(filter(lambda x: '.out' in x, annts))\n",
    "    raws = sorted(os.listdir(raw_path))\n",
    "    raws = list(filter(lambda x: '.txt' in x, raws))\n",
    "    assert len(annts) == len(raws)\n",
    "\n",
    "    results = []\n",
    "    for a,r in tqdm(zip(annts, raws), total=len(raws)):\n",
    "        aname = a.split('.')[0]\n",
    "        rname = r.split('.')[0]\n",
    "        assert aname == rname\n",
    "        apath = os.path.join(annt_path, a)\n",
    "        rpath = os.path.join(raw_path, r)\n",
    "        results += merge_sents_and_ents(snlp, nlp, rpath, apath)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ner_bio_file():\n",
    "    def write_sents(file_name, sents):\n",
    "        print(f'writing {len(sents)} sentences to {file_name}')\n",
    "        f = open(file_name, 'w', encoding='utf8')\n",
    "        for sent in sents:\n",
    "            for token, label in sent:\n",
    "                f.write(token+' '+label+'\\n')\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "    \n",
    "    import random\n",
    "    sents1 = extract_ner_bio_sents('test')\n",
    "    sents2 = extract_ner_bio_sents('train')\n",
    "    all_sents = sents1 + sents2\n",
    "    random.shuffle(all_sents)\n",
    "    total = len(sents1) + len(sents2)\n",
    "    train_count = int(total*0.75)\n",
    "    dev_count = int(total*0.10)\n",
    "    train_sents = []\n",
    "    dev_sents = []\n",
    "    test_sents = []\n",
    "    count = 0\n",
    "    for sent in all_sents:\n",
    "        if count < train_count:\n",
    "            train_sents.append(sent)\n",
    "        elif count <train_count + dev_count:\n",
    "            dev_sents.append(sent)\n",
    "        else:\n",
    "            test_sents.append(sent)\n",
    "        count += 1\n",
    "    \n",
    "    write_sents('all.txt', all_sents)\n",
    "    write_sents('train.txt', train_sents)\n",
    "    write_sents('test.txt', test_sents)\n",
    "    write_sents('dev.txt', dev_sents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing 6510 sentences to all.txt\n",
      "writing 4882 sentences to train.txt\n",
      "writing 977 sentences to test.txt\n",
      "writing 651 sentences to dev.txt\n"
     ]
    }
   ],
   "source": [
    "write_ner_bio_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
